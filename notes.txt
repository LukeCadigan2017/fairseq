Ok, so looks like training works with original parameters

Now, we're trying to figure out how to train it with sparsemax

fconv_wmt_en_fr

Looks like when you train, you specify architecture with --arch and checkpoint dir with --save-dir 



ls fairseq/models/transformer/transformer_decoder.py
ls fairseq/modules/multihead_attention.py

cp fairseq/models/transformer/transformer_decoder.py fairseq/models/transformer/transformer_decoder_original.py
cp fairseq/modules/multihead_attention.py fairseq/modules/multihead_attention_original.py

cp fairseq/models/transformer/transformer_decoder.py fairseq/models/transformer/transformer_decoder_multimax.py
cp fairseq/modules/multihead_attention.py fairseq/modules/multihead_attention_multimax.py

    


cp /Users/lukecadigan/thesis/MultiMax/thesis/transformer_decoder.py fairseq/models/transformer/transformer_decoder.py
cp /Users/lukecadigan/thesis/MultiMax/thesis/multihead_attention.py fairseq/modules/multihead_attention.py




code fairseq/models/transformer/transformer_decoder.py
code fairseq/modules/multihead_attention.py

