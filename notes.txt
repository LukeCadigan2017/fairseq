Changes I made:
    Two changes in the attention stuff
    Change in the output layer


Important jobs:
    vi ./outputs/fairseq_train_12462921.out


------------------------------------------------------------------------------------------------------------------------------------------------

Ok, so looks like training works with original parameters

Now, we're trying to figure out how to train it with sparsemax

fconv_wmt_en_fr

Looks like when you train, you specify architecture with --arch and checkpoint dir with --save-dir 

export FOREIGN_PATH=/gpfs/home5/lcadigan/fairseq/outputs/fairseq_train_12462921.out
export LOCAL_NAME=fairseq_train_softmax.out
rsync -av lcadigan@snellius.surf.nl:${FOREIGN_PATH} ${LOCAL_NAME}




export FOREIGN_PATH=/gpfs/home5/lcadigan/fairseq/outputs/fairseq_train_12480089.out
export LOCAL_NAME=fairseq_train_softmax2.out
rsync -av lcadigan@snellius.surf.nl:${FOREIGN_PATH} ${LOCAL_NAME}


Ok, averaging 5:
BLEU4 = 27.67, 58.9/33.4/21.3/14.0 (BP=1.000, ratio=1.008, syslen=64855, reflen=64355)
Averaging 10:
BLEU4 = 27.70, 59.0/33.4/21.3/14.0 (BP=1.000, ratio=1.008, syslen=64862, reflen=64355)


Random loss for sparsemax: loss size is 12333.8212890625